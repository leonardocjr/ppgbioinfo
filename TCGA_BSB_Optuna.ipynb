{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação e importação de Bibliotecas\n",
    "# pip install pandas numpy matplotlib scikit-learn optuna tensorflow keras\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import optuna\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, SpatialDropout1D, SimpleRNN, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento dos dados\n",
    "df = pd.read_csv(\"C:/Users/ana_v/OneDrive/Documentos/Mestrado/MachineLearning/TCGA.csv\", low_memory=False)\n",
    "unique_types = df['Type'].unique()\n",
    "type_to_numeric = {type_name: index for index, type_name in enumerate(unique_types)}\n",
    "df['Type'] = df['Type'].map(type_to_numeric)\n",
    "type_column = df['Type']\n",
    "df_num = df.drop(columns=[\"Sample\", \"Type\"])\n",
    "\n",
    "# Normalização dos dados\n",
    "scaler = StandardScaler()\n",
    "dados_normalizados = scaler.fit_transform(df_num)\n",
    "\n",
    "# Aplicação do PCA\n",
    "pca = PCA(n_components=0.8)  \n",
    "pca.fit(dados_normalizados)\n",
    "dados_pca = pca.transform(dados_normalizados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(dados_pca, df['Type'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Função objetivo para otimização do MLP\n",
    "def objective_mlp(trial):\n",
    "    # Hiperparâmetros para MLP\n",
    "    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (100, 50), (50, 25)])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic'])\n",
    "    solver = trial.suggest_categorical('solver', ['adam', 'sgd'])\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
    "    learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
    "    \n",
    "    # Criar e treinar o modelo\n",
    "    model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation,\n",
    "                          solver=solver, alpha=alpha, learning_rate_init=learning_rate_init,\n",
    "                          max_iter=500, random_state=42)\n",
    "    \n",
    "    # Validação cruzada\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "# Estudar otimização com Optuna para MLP\n",
    "study_mlp = optuna.create_study(direction='maximize')\n",
    "study_mlp.optimize(objective_mlp, n_trials=10)\n",
    "\n",
    "# Melhores parâmetros\n",
    "best_params_mlp = study_mlp.best_params\n",
    "print(\"Melhores parâmetros para MLP:\", best_params_mlp)\n",
    "\n",
    "# Treinamento e avaliação do MLP com melhores parâmetros\n",
    "best_mlp_model = MLPClassifier(hidden_layer_sizes=best_params_mlp['hidden_layer_sizes'],\n",
    "                               activation=best_params_mlp['activation'],\n",
    "                               solver=best_params_mlp['solver'],\n",
    "                               alpha=best_params_mlp['alpha'],\n",
    "                               learning_rate_init=best_params_mlp['learning_rate_init'],\n",
    "                               max_iter=500, random_state=42)\n",
    "best_mlp_model.fit(X_train, y_train)\n",
    "mlp_predictions_train = best_mlp_model.predict(X_train)\n",
    "mlp_predictions_test = best_mlp_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação para MLP\n",
    "print(\"\\nResultados do MLP:\")\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_test, mlp_predictions_test))\n",
    "print(\"Acurácia (Treinamento):\", accuracy_score(y_train, mlp_predictions_train))\n",
    "print(\"Acurácia (Teste):\", accuracy_score(y_test, mlp_predictions_test))\n",
    "\n",
    "# Acurácia média na validação cruzada\n",
    "cv_mean_score = cross_val_score(best_mlp_model, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "print(f'Acurácia média na validação cruzada (MLP): {cv_mean_score:.4f}')\n",
    "\n",
    "# Classification Report\n",
    "mlp_report = classification_report(y_test, mlp_predictions_test, zero_division=1, target_names=[str(cls) for cls in unique_types])\n",
    "print(f'\\nClassification Report (MLP):\\n{mlp_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir número de classes\n",
    "num_classes = len(unique_types) \n",
    "\n",
    "# Função para criar o modelo CNN\n",
    "def create_cnn_model(filters=32, kernel_size=3, pool_size=2, dense_units=64, dropout_rate=0.5):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        MaxPooling1D(pool_size=pool_size),\n",
    "        Conv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu'),\n",
    "        MaxPooling1D(pool_size=pool_size),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Função objetivo para otimização do CNN\n",
    "def objective_cnn(trial):\n",
    "    # Hiperparâmetros para CNN\n",
    "    filters = trial.suggest_int('filters', 16, 64)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 2, 5)\n",
    "    pool_size = trial.suggest_int('pool_size', 2, 4)\n",
    "    dense_units = trial.suggest_int('dense_units', 32, 128)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
    "\n",
    "    # Criação do modelo\n",
    "    model = create_cnn_model(filters=filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             pool_size=pool_size,\n",
    "                             dense_units=dense_units,\n",
    "                             dropout_rate=dropout_rate)\n",
    "\n",
    "    # Validação cruzada\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, X_fold_val = X_train[train_index], X_train[val_index]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train, epochs=10, batch_size=32, verbose=0)\n",
    "        val_loss, val_accuracy = model.evaluate(X_fold_val, y_fold_val, verbose=0)\n",
    "        cv_scores.append(val_accuracy)\n",
    "    \n",
    "    # Média da acurácia de validação cruzada\n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "\n",
    "    return mean_cv_accuracy\n",
    "\n",
    "# Estudar otimização com Optuna para CNN\n",
    "study_cnn = optuna.create_study(direction='maximize')\n",
    "study_cnn.optimize(objective_cnn, n_trials=10)\n",
    "\n",
    "# Melhores parâmetros\n",
    "best_params_cnn = study_cnn.best_params\n",
    "print(\"Melhores parâmetros para CNN:\", best_params_cnn)\n",
    "\n",
    "# Treinamento e avaliação do CNN com melhores parâmetros\n",
    "best_cnn_model = create_cnn_model(filters=best_params_cnn['filters'],\n",
    "                                  kernel_size=best_params_cnn['kernel_size'],\n",
    "                                  pool_size=best_params_cnn['pool_size'],\n",
    "                                  dense_units=best_params_cnn['dense_units'],\n",
    "                                  dropout_rate=best_params_cnn['dropout_rate'])\n",
    "\n",
    "# Treinamento do modelo\n",
    "history = best_cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Avaliação da CNN no conjunto de teste\n",
    "cnn_loss, cnn_accuracy = best_cnn_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Predições da CNN no conjunto de teste\n",
    "cnn_predictions = best_cnn_model.predict(X_test)\n",
    "cnn_predictions_classes = np.argmax(cnn_predictions, axis=1)\n",
    "\n",
    "# Classification report da CNN\n",
    "cnn_report = classification_report(y_test, cnn_predictions_classes, target_names=[str(cls) for cls in unique_types], zero_division=1, digits=2)\n",
    "\n",
    "# Matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test, cnn_predictions_classes)\n",
    "\n",
    "# Acurácia de treinamento\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "\n",
    "# Acurácia da validação cruzada (a média das acurácias obtidas durante a otimização)\n",
    "cv_mean_accuracy = study_cnn.best_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo resultados da CNN\n",
    "print(f'\\nCNN - Acurácia (Treinamento): {train_accuracy:.4f}')\n",
    "print(f'CNN - Acurácia (Teste): {cnn_accuracy:.4f}')\n",
    "print(f'Acurácia média na validação cruzada (CNN): {cv_mean_accuracy:.4f}')\n",
    "print(f'\\nMatriz de Confusão (CNN):\\n{conf_matrix}')\n",
    "print(f'\\nClassification Report (CNN):\\n{cnn_report}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
