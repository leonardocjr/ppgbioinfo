{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação e importação de Bibliotecas\n",
    "# pip install pandas numpy matplotlib scikit-learn optuna tensorflow keras seaborn\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import optuna\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, SpatialDropout1D, SimpleRNN, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento dos dados\n",
    "df = pd.read_csv(\"C:/Users/ana_v/OneDrive/Documentos/Mestrado/MachineLearning/TCGA.csv\", low_memory=False)\n",
    "unique_types = df['Type'].unique()\n",
    "type_to_numeric = {type_name: index for index, type_name in enumerate(unique_types)}\n",
    "df['Type'] = df['Type'].map(type_to_numeric)\n",
    "type_column = df['Type']\n",
    "df_num = df.drop(columns=[\"Sample\", \"Type\"])\n",
    "\n",
    "# Normalização dos dados\n",
    "scaler = StandardScaler()\n",
    "dados_normalizados = scaler.fit_transform(df_num)\n",
    "\n",
    "# Aplicação do PCA\n",
    "pca = PCA(n_components=0.8)  \n",
    "pca.fit(dados_normalizados)\n",
    "dados_pca = pca.transform(dados_normalizados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-26 15:32:25,764] A new study created in memory with name: no-name-b1a1a781-eab3-4985-b90e-e4ecdacefc6b\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
      "[I 2024-08-26 15:32:28,977] Trial 0 finished with value: 0.5215391498881432 and parameters: {'n_layers': 4, 'n_units_l0': 115, 'n_units_l1': 25, 'n_units_l2': 63, 'n_units_l3': 43, 'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.00047509237210306113, 'learning_rate_init': 0.011367330868956235}. Best is trial 0 with value: 0.5215391498881432.\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2024-08-26 15:32:49,279] Trial 1 finished with value: 0.7486800894854586 and parameters: {'n_layers': 3, 'n_units_l0': 135, 'n_units_l1': 28, 'n_units_l2': 109, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0746528346269155, 'learning_rate_init': 0.0008715103205978142}. Best is trial 1 with value: 0.7486800894854586.\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
      "[I 2024-08-26 15:32:55,865] Trial 2 finished with value: 0.7260134228187919 and parameters: {'n_layers': 5, 'n_units_l0': 135, 'n_units_l1': 137, 'n_units_l2': 35, 'n_units_l3': 29, 'n_units_l4': 46, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.005854751355295724, 'learning_rate_init': 0.0008841926348917736}. Best is trial 1 with value: 0.7486800894854586.\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
      "[I 2024-08-26 15:33:53,698] Trial 3 finished with value: 0.7219686800894854 and parameters: {'n_layers': 5, 'n_units_l0': 130, 'n_units_l1': 27, 'n_units_l2': 119, 'n_units_l3': 149, 'n_units_l4': 119, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0001494364677378836, 'learning_rate_init': 0.0007300053042915264}. Best is trial 1 with value: 0.7486800894854586.\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2024-08-26 15:34:12,691] Trial 4 finished with value: 0.7594004474272931 and parameters: {'n_layers': 2, 'n_units_l0': 27, 'n_units_l1': 110, 'activation': 'logistic', 'solver': 'sgd', 'alpha': 3.862907418494322e-05, 'learning_rate_init': 0.005860256303804338}. Best is trial 4 with value: 0.7594004474272931.\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
      "[I 2024-08-26 15:34:30,955] Trial 5 finished with value: 0.6804742729306488 and parameters: {'n_layers': 5, 'n_units_l0': 37, 'n_units_l1': 77, 'n_units_l2': 112, 'n_units_l3': 77, 'n_units_l4': 31, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.04107784588077006, 'learning_rate_init': 0.0002584783141380735}. Best is trial 4 with value: 0.7594004474272931.\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
      "[I 2024-08-26 15:34:38,170] Trial 6 finished with value: 0.7607606263982103 and parameters: {'n_layers': 2, 'n_units_l0': 126, 'n_units_l1': 75, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.034136922393519724, 'learning_rate_init': 0.007430475750099704}. Best is trial 6 with value: 0.7607606263982103.\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
      "[I 2024-08-26 15:34:43,209] Trial 7 finished with value: 0.7047069351230425 and parameters: {'n_layers': 5, 'n_units_l0': 68, 'n_units_l1': 59, 'n_units_l2': 137, 'n_units_l3': 78, 'n_units_l4': 146, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.002058535957997779, 'learning_rate_init': 0.0016765264451568493}. Best is trial 6 with value: 0.7607606263982103.\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
      "[I 2024-08-26 15:34:56,692] Trial 8 finished with value: 0.7714362416107383 and parameters: {'n_layers': 3, 'n_units_l0': 138, 'n_units_l1': 97, 'n_units_l2': 25, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.04306660168805714, 'learning_rate_init': 0.007414474013266335}. Best is trial 8 with value: 0.7714362416107383.\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\2507079711.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
      "[I 2024-08-26 15:35:05,556] Trial 9 finished with value: 0.7300223713646532 and parameters: {'n_layers': 2, 'n_units_l0': 142, 'n_units_l1': 112, 'activation': 'relu', 'solver': 'adam', 'alpha': 1.8365412649690993e-05, 'learning_rate_init': 0.018466694211779083}. Best is trial 8 with value: 0.7714362416107383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros para MLP: {'n_layers': 3, 'n_units_l0': 138, 'n_units_l1': 97, 'n_units_l2': 25, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.04306660168805714, 'learning_rate_init': 0.007414474013266335}\n"
     ]
    }
   ],
   "source": [
    "# Fixar a seed para garantir a reprodutibilidade\n",
    "seed = 1\n",
    "\n",
    "# Divisão de treino e teste com random_state\n",
    "X_train, X_test, y_train, y_test = train_test_split(dados_pca, df['Type'], test_size=0.2, random_state=seed)\n",
    "\n",
    "# Função objetivo para otimização do MLP\n",
    "def objective_mlp(trial):\n",
    "    # Sugestão do número de camadas ocultas (entre 1 e 6 camadas)\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 6)\n",
    "    \n",
    "    # Sugestão do número de neurônios em cada camada\n",
    "    hidden_layer_sizes = [trial.suggest_int(f'n_units_l{i}', 25, 150) for i in range(n_layers)]\n",
    "    \n",
    "    # Outros hiperparâmetros\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic'])\n",
    "    solver = trial.suggest_categorical('solver', ['adam', 'sgd'])\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
    "    learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1)\n",
    "    \n",
    "    # Criar e treinar o modelo\n",
    "    model = MLPClassifier(hidden_layer_sizes=tuple(hidden_layer_sizes), activation=activation,\n",
    "                          solver=solver, alpha=alpha, learning_rate_init=learning_rate_init,\n",
    "                          max_iter=500, random_state=seed)\n",
    "    \n",
    "    # Validação cruzada com random_state para reprodutibilidade\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "# Estudo de otimização com Optuna para MLP com seed fixa\n",
    "study_mlp = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "study_mlp.optimize(objective_mlp, n_trials=10)\n",
    "\n",
    "# Melhores parâmetros\n",
    "best_params_mlp = study_mlp.best_params\n",
    "print(\"Melhores parâmetros para MLP:\", best_params_mlp)\n",
    "\n",
    "# Treinamento e avaliação do MLP com os melhores parâmetros\n",
    "best_mlp_model = MLPClassifier(hidden_layer_sizes=tuple(best_params_mlp[f'n_units_l{i}'] for i in range(best_params_mlp['n_layers'])),\n",
    "                               activation=best_params_mlp['activation'],\n",
    "                               solver=best_params_mlp['solver'],\n",
    "                               alpha=best_params_mlp['alpha'],\n",
    "                               learning_rate_init=best_params_mlp['learning_rate_init'],\n",
    "                               max_iter=500, random_state=seed)\n",
    "best_mlp_model.fit(X_train, y_train)\n",
    "mlp_predictions_train = best_mlp_model.predict(X_train)\n",
    "mlp_predictions_test = best_mlp_model.predict(X_test)\n",
    "\n",
    "# Avaliação do modelo\n",
    "mlp_accuracy_train = accuracy_score(y_train, mlp_predictions_train)\n",
    "mlp_accuracy_test = accuracy_score(y_test, mlp_predictions_test)\n",
    "mlp_report = classification_report(y_test, mlp_predictions_test, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados do MLP:\n",
      "Matriz de Confusão:\n",
      " [[82  4  0  2  2]\n",
      " [13 26  0  2  0]\n",
      " [ 0  1 27  1  2]\n",
      " [ 0  7  2 11  1]\n",
      " [ 2  0  0  0  2]]\n",
      "Acurácia (Treinamento): 1.0\n",
      "Acurácia (Teste): 0.7914438502673797\n",
      "Acurácia média na validação cruzada (MLP): 0.7794\n",
      "\n",
      "Classification Report (MLP):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88        90\n",
      "           1       0.68      0.63      0.66        41\n",
      "           2       0.93      0.87      0.90        31\n",
      "           3       0.69      0.52      0.59        21\n",
      "           4       0.29      0.50      0.36         4\n",
      "\n",
      "    accuracy                           0.79       187\n",
      "   macro avg       0.69      0.69      0.68       187\n",
      "weighted avg       0.79      0.79      0.79       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Avaliação para MLP\n",
    "print(\"\\nResultados do MLP:\")\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_test, mlp_predictions_test))\n",
    "print(\"Acurácia (Treinamento):\", accuracy_score(y_train, mlp_predictions_train))\n",
    "print(\"Acurácia (Teste):\", accuracy_score(y_test, mlp_predictions_test))\n",
    "\n",
    "# Acurácia média na validação cruzada\n",
    "cv_mean_score = cross_val_score(best_mlp_model, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "print(f'Acurácia média na validação cruzada (MLP): {cv_mean_score:.4f}')\n",
    "\n",
    "# Classification Report\n",
    "print(f'\\nClassification Report (MLP):\\n{mlp_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-26 15:35:56,589] A new study created in memory with name: no-name-7dfaa4b7-a6ea-40ed-9f1d-310d03eb3c7c\n",
      "C:\\Users\\ana_v\\AppData\\Local\\Temp\\ipykernel_9896\\3313243246.py:41: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "[I 2024-08-26 15:36:03,848] Trial 0 finished with value: 0.8158389210700989 and parameters: {'n_conv_layers': 2, 'n_dense_layers': 3, 'filters': 36, 'kernel_size': 2, 'pool_size': 2, 'dense_units': 111, 'dropout_rate': 0.29756163591894164, 'epochs': 5}. Best is trial 0 with value: 0.8158389210700989.\n",
      "[I 2024-08-26 15:36:16,926] Trial 1 finished with value: 0.9053333282470704 and parameters: {'n_conv_layers': 2, 'n_dense_layers': 2, 'filters': 39, 'kernel_size': 4, 'pool_size': 2, 'dense_units': 112, 'dropout_rate': 0.29788533249844734, 'epochs': 13}. Best is trial 1 with value: 0.9053333282470704.\n",
      "[I 2024-08-26 15:36:25,960] Trial 2 finished with value: 0.8318657755851746 and parameters: {'n_conv_layers': 4, 'n_dense_layers': 2, 'filters': 17, 'kernel_size': 2, 'pool_size': 2, 'dense_units': 69, 'dropout_rate': 0.3060017275619985, 'epochs': 6}. Best is trial 1 with value: 0.9053333282470704.\n",
      "[I 2024-08-26 15:36:40,524] Trial 3 finished with value: 0.8972885847091675 and parameters: {'n_conv_layers': 3, 'n_dense_layers': 2, 'filters': 35, 'kernel_size': 5, 'pool_size': 2, 'dense_units': 114, 'dropout_rate': 0.300584912696117, 'epochs': 10}. Best is trial 1 with value: 0.9053333282470704.\n",
      "[I 2024-08-26 15:36:52,261] Trial 4 finished with value: 0.7114899277687072 and parameters: {'n_conv_layers': 1, 'n_dense_layers': 3, 'filters': 40, 'kernel_size': 2, 'pool_size': 3, 'dense_units': 102, 'dropout_rate': 0.3914040985520284, 'epochs': 13}. Best is trial 1 with value: 0.9053333282470704.\n",
      "[I 2024-08-26 15:37:02,791] Trial 5 finished with value: 0.8986487746238708 and parameters: {'n_conv_layers': 2, 'n_dense_layers': 1, 'filters': 40, 'kernel_size': 4, 'pool_size': 3, 'dense_units': 84, 'dropout_rate': 0.35596943106501006, 'epochs': 9}. Best is trial 1 with value: 0.9053333282470704.\n",
      "[I 2024-08-26 15:37:42,199] Trial 6 finished with value: 0.9079463243484497 and parameters: {'n_conv_layers': 4, 'n_dense_layers': 1, 'filters': 51, 'kernel_size': 4, 'pool_size': 2, 'dense_units': 112, 'dropout_rate': 0.2366964750056567, 'epochs': 15}. Best is trial 6 with value: 0.9079463243484497.\n",
      "[I 2024-08-26 15:37:52,926] Trial 7 finished with value: 0.8986308693885803 and parameters: {'n_conv_layers': 3, 'n_dense_layers': 1, 'filters': 56, 'kernel_size': 3, 'pool_size': 3, 'dense_units': 80, 'dropout_rate': 0.2392359799535047, 'epochs': 12}. Best is trial 6 with value: 0.9079463243484497.\n",
      "[I 2024-08-26 15:38:14,230] Trial 8 finished with value: 0.9253064870834351 and parameters: {'n_conv_layers': 2, 'n_dense_layers': 1, 'filters': 55, 'kernel_size': 4, 'pool_size': 2, 'dense_units': 128, 'dropout_rate': 0.4536719286586085, 'epochs': 20}. Best is trial 8 with value: 0.9253064870834351.\n",
      "[I 2024-08-26 15:38:22,408] Trial 9 finished with value: 0.8612706899642945 and parameters: {'n_conv_layers': 2, 'n_dense_layers': 1, 'filters': 18, 'kernel_size': 2, 'pool_size': 3, 'dense_units': 65, 'dropout_rate': 0.34239558411766813, 'epochs': 15}. Best is trial 8 with value: 0.9253064870834351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros para CNN: {'n_conv_layers': 2, 'n_dense_layers': 1, 'filters': 55, 'kernel_size': 4, 'pool_size': 2, 'dense_units': 128, 'dropout_rate': 0.4536719286586085, 'epochs': 20}\n",
      "Epoch 1/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.3669 - loss: 2.5761 - val_accuracy: 0.4813 - val_loss: 1.2751\n",
      "Epoch 2/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6033 - loss: 1.1094 - val_accuracy: 0.6738 - val_loss: 0.9236\n",
      "Epoch 3/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7446 - loss: 0.7788 - val_accuracy: 0.6471 - val_loss: 0.8675\n",
      "Epoch 4/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7837 - loss: 0.6707 - val_accuracy: 0.7059 - val_loss: 0.7503\n",
      "Epoch 5/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8473 - loss: 0.4418 - val_accuracy: 0.7326 - val_loss: 0.6982\n",
      "Epoch 6/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8848 - loss: 0.3433 - val_accuracy: 0.7594 - val_loss: 0.6649\n",
      "Epoch 7/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9332 - loss: 0.2382 - val_accuracy: 0.8021 - val_loss: 0.6476\n",
      "Epoch 8/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9312 - loss: 0.2313 - val_accuracy: 0.7861 - val_loss: 0.6610\n",
      "Epoch 9/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9329 - loss: 0.2055 - val_accuracy: 0.7861 - val_loss: 0.7162\n",
      "Epoch 10/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9540 - loss: 0.1521 - val_accuracy: 0.7647 - val_loss: 0.7937\n",
      "Epoch 11/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9530 - loss: 0.1339 - val_accuracy: 0.7594 - val_loss: 0.8295\n",
      "Epoch 12/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9638 - loss: 0.1459 - val_accuracy: 0.7647 - val_loss: 0.8570\n",
      "Epoch 13/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9656 - loss: 0.1019 - val_accuracy: 0.7433 - val_loss: 0.9151\n",
      "Epoch 14/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9703 - loss: 0.0913 - val_accuracy: 0.7540 - val_loss: 1.1086\n",
      "Epoch 15/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9705 - loss: 0.0755 - val_accuracy: 0.7647 - val_loss: 0.9597\n",
      "Epoch 16/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9654 - loss: 0.0851 - val_accuracy: 0.7594 - val_loss: 0.9708\n",
      "Epoch 17/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9826 - loss: 0.0624 - val_accuracy: 0.7647 - val_loss: 1.0183\n",
      "Epoch 18/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9906 - loss: 0.0494 - val_accuracy: 0.7380 - val_loss: 1.0278\n",
      "Epoch 19/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9944 - loss: 0.0419 - val_accuracy: 0.7219 - val_loss: 1.2102\n",
      "Epoch 20/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9857 - loss: 0.0525 - val_accuracy: 0.7594 - val_loss: 1.2368\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7777 - loss: 1.1925 \n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    }
   ],
   "source": [
    "# Definir número de classes\n",
    "num_classes = len(unique_types)\n",
    "\n",
    "# Função para criar o modelo CNN com flexibilidade no número de camadas\n",
    "def create_cnn_model(n_conv_layers=2, n_dense_layers=1, filters=32, kernel_size=3, pool_size=2, dense_units=64, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adicionar camadas convolucionais conforme definido por Optuna\n",
    "    for i in range(n_conv_layers):\n",
    "        if i == 0:\n",
    "            # Primeira camada convolucional, precisa definir o input_shape\n",
    "            model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same', input_shape=(X_train.shape[1], 1)))\n",
    "        else:\n",
    "            model.add(Conv1D(filters=filters * (2 ** i), kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Adicionar camadas densas conforme definido por Optuna\n",
    "    for _ in range(n_dense_layers):\n",
    "        model.add(Dense(dense_units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Camada de saída\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Função objetivo para otimização do CNN com Optuna\n",
    "def objective_cnn(trial):\n",
    "    # Hiperparâmetros para CNN com intervalos mais restritos\n",
    "    n_conv_layers = trial.suggest_int('n_conv_layers', 1, 4)  # Número de camadas convolucionais\n",
    "    n_dense_layers = trial.suggest_int('n_dense_layers', 1, 3)  # Número de camadas densas\n",
    "    filters = trial.suggest_int('filters', 16, 64)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 2, 5)  # Intervalo ajustado\n",
    "    pool_size = trial.suggest_int('pool_size', 2, 3)  # Intervalo ajustado\n",
    "    dense_units = trial.suggest_int('dense_units', 32, 128)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
    "    epochs = trial.suggest_int('epochs', 5, 20)  # Número de épocas para otimização\n",
    "\n",
    "    # Criação do modelo\n",
    "    model = create_cnn_model(n_conv_layers=n_conv_layers,\n",
    "                             n_dense_layers=n_dense_layers,\n",
    "                             filters=filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             pool_size=pool_size,\n",
    "                             dense_units=dense_units,\n",
    "                             dropout_rate=dropout_rate)\n",
    "\n",
    "    # Validação cruzada\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    cv_scores = []\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_fold_train, X_fold_val = X_train[train_index], X_train[val_index]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train, epochs=epochs, batch_size=32, verbose=0)\n",
    "        val_loss, val_accuracy = model.evaluate(X_fold_val, y_fold_val, verbose=0)\n",
    "        cv_scores.append(val_accuracy)\n",
    "    \n",
    "    # Média da acurácia de validação cruzada\n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "\n",
    "    return mean_cv_accuracy\n",
    "\n",
    "# Estudo de otimização com Optuna para CNN\n",
    "study_cnn = optuna.create_study(direction='maximize')\n",
    "study_cnn.optimize(objective_cnn, n_trials=10)\n",
    "\n",
    "# Melhores parâmetros\n",
    "best_params_cnn = study_cnn.best_params\n",
    "print(\"Melhores parâmetros para CNN:\", best_params_cnn)\n",
    "\n",
    "# Treinamento e avaliação do CNN com melhores parâmetros\n",
    "best_cnn_model = create_cnn_model(n_conv_layers=best_params_cnn['n_conv_layers'],\n",
    "                                  n_dense_layers=best_params_cnn['n_dense_layers'],\n",
    "                                  filters=best_params_cnn['filters'],\n",
    "                                  kernel_size=best_params_cnn['kernel_size'],\n",
    "                                  pool_size=best_params_cnn['pool_size'],\n",
    "                                  dense_units=best_params_cnn['dense_units'],\n",
    "                                  dropout_rate=best_params_cnn['dropout_rate'])\n",
    "\n",
    "# Treinamento do modelo com o melhor número de épocas\n",
    "history = best_cnn_model.fit(X_train, y_train, epochs=best_params_cnn['epochs'], batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Avaliação da CNN no conjunto de teste\n",
    "cnn_loss, cnn_accuracy = best_cnn_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Predições da CNN no conjunto de teste\n",
    "cnn_predictions = best_cnn_model.predict(X_test)\n",
    "cnn_predictions_classes = np.argmax(cnn_predictions, axis=1)\n",
    "\n",
    "# Classification report da CNN\n",
    "cnn_report = classification_report(y_test, cnn_predictions_classes, target_names=[str(cls) for cls in unique_types], zero_division=1, digits=2)\n",
    "\n",
    "# Matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test, cnn_predictions_classes)\n",
    "\n",
    "# Acurácia de treinamento\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "\n",
    "# Acurácia da validação cruzada \n",
    "cv_mean_accuracy = study_cnn.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN - Acurácia (Treinamento): 0.9799\n",
      "CNN - Acurácia (Teste): 0.7594\n",
      "Acurácia média na validação cruzada (CNN): 0.9253\n",
      "\n",
      "Matriz de Confusão (CNN):\n",
      "[[82  5  1  2  0]\n",
      " [17 21  1  2  0]\n",
      " [ 2  0 29  0  0]\n",
      " [ 5  5  2  9  0]\n",
      " [ 3  0  0  0  1]]\n",
      "\n",
      "Classification Report (CNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        LumA       0.75      0.91      0.82        90\n",
      "        LumB       0.68      0.51      0.58        41\n",
      "       Basal       0.88      0.94      0.91        31\n",
      "        Her2       0.69      0.43      0.53        21\n",
      "      Normal       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.76       187\n",
      "   macro avg       0.80      0.61      0.65       187\n",
      "weighted avg       0.76      0.76      0.74       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibindo resultados da CNN\n",
    "print(f'\\nCNN - Acurácia (Treinamento): {train_accuracy:.4f}')\n",
    "print(f'CNN - Acurácia (Teste): {cnn_accuracy:.4f}')\n",
    "print(f'Acurácia média na validação cruzada (CNN): {cv_mean_accuracy:.4f}')\n",
    "print(f'\\nMatriz de Confusão (CNN):\\n{conf_matrix}')\n",
    "print(f'\\nClassification Report (CNN):\\n{cnn_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir unique_types e target_names\n",
    "unique_types = df['Type'].unique()\n",
    "target_names = [str(cls) for cls in unique_types]\n",
    "\n",
    "# Matrizes de confusão\n",
    "conf_matrix_cnn = confusion_matrix(y_test, cnn_predictions_classes)\n",
    "conf_matrix_mlp = confusion_matrix(y_test, mlp_predictions_test)\n",
    "\n",
    "# Função para plotar e salvar a matriz de confusão\n",
    "def plot_confusion_matrix(conf_matrix, title, filename):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greys', \n",
    "                xticklabels=target_names, yticklabels=target_names, \n",
    "                annot_kws={\"size\": 16})  \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Plotar e salvar a matriz de confusão para CNN\n",
    "plot_confusion_matrix(conf_matrix_cnn, 'Confusion Matrix - CNN', 'confusion_matrix_cnn_op.png')\n",
    "\n",
    "# Plotar e salvar a matriz de confusão para MLP\n",
    "plot_confusion_matrix(conf_matrix_mlp, 'Confusion Matrix - MLP', 'confusion_matrix_mlp_op.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
