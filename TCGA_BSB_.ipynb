{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, SpatialDropout1D, SimpleRNN, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "df = pd.read_csv(\"C:/Users/ana_v/OneDrive/Documentos/Mestrado/MachineLearning/TCGA.csv\", low_memory=False)\n",
    "\n",
    "# Obtém os valores únicos na coluna 'Type'\n",
    "unique_types = df['Type'].unique()\n",
    "\n",
    "# Cria um dicionário mapeando cada tipo único para um número\n",
    "type_to_numeric = {type_name: index for index, type_name in enumerate(unique_types)}\n",
    "\n",
    "# Aplica a substituição usando o método map\n",
    "df['Type'] = df['Type'].map(type_to_numeric)\n",
    "\n",
    "# Armazena a coluna 'Type' para adicioná-la de volta posteriormente\n",
    "type_column = df['Type']\n",
    "\n",
    "# Prepara o DataFrame para normalização (remover colunas desnecessárias)\n",
    "df_num = df.drop(columns=[\"Sample\", \"Type\"])\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "dados_normalizados = scaler.fit_transform(df_num)\n",
    "\n",
    "# Aplicação do PCA\n",
    "pca = PCA(n_components=0.8)  \n",
    "pca.fit(dados_normalizados)\n",
    "dados_pca = pca.transform(dados_normalizados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - Acurácia (Treinamento): 1.0\n",
      "MLP - Acurácia (Teste): 0.732620320855615\n",
      "Acurácia média na validação cruzada (MLP): 0.7294097460535347\n",
      "\n",
      "Classification Report MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.82        97\n",
      "           1       0.56      0.63      0.59        38\n",
      "           2       0.88      0.79      0.84        29\n",
      "           3       0.56      0.60      0.58        15\n",
      "           4       0.33      0.25      0.29         8\n",
      "\n",
      "    accuracy                           0.73       187\n",
      "   macro avg       0.63      0.62      0.62       187\n",
      "weighted avg       0.74      0.73      0.73       187\n",
      "\n",
      "Matriz de Confusão:\n",
      " [[79 15  1  2  0]\n",
      " [11 24  1  2  0]\n",
      " [ 2  1 23  1  2]\n",
      " [ 0  3  1  9  2]\n",
      " [ 4  0  0  2  2]]\n"
     ]
    }
   ],
   "source": [
    "# Divisão de treino e teste com a função random_state usada para garantir a reprodutibilidade dos resultados\n",
    "X_train, X_test, y_train, y_test = train_test_split(dados_pca, df['Type'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Criação e treinamento do MLP\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predições no conjunto de treino e teste\n",
    "mlp_predictions_train = mlp_model.predict(X_train)\n",
    "mlp_predictions_test = mlp_model.predict(X_test)\n",
    "\n",
    "# Avaliação MLP\n",
    "mlp_accuracy_train = accuracy_score(y_train, mlp_predictions_train)\n",
    "mlp_accuracy_test = accuracy_score(y_test, mlp_predictions_test)\n",
    "mlp_report = classification_report(y_test, mlp_predictions_test, zero_division=1)\n",
    "\n",
    "# Criar um objeto de validação cruzada\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Substituir a divisão de treino e teste pelo código de validação cruzada\n",
    "mlp_scores = cross_val_score(mlp_model, dados_pca, df['Type'], cv=cv, scoring='accuracy')\n",
    "\n",
    "# Exibindo resultados do MLP\n",
    "print(f'MLP - Acurácia (Treinamento): {mlp_accuracy_train}')\n",
    "print(f'MLP - Acurácia (Teste): {mlp_accuracy_test}')\n",
    "print(f'Acurácia média na validação cruzada (MLP): {mlp_scores.mean()}')\n",
    "print('')\n",
    "print(f'Classification Report MLP:\\n{mlp_report}')\n",
    "\n",
    "# Defina as classes com base nos seus dados\n",
    "classes = df['Type'].unique()\n",
    "\n",
    "# Matriz de Confusão do MLP\n",
    "# Avaliação para MLP\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_test, mlp_predictions_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (748, 184, 1)\n",
      "Shape of X_test: (187, 184, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ana_v\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.3611 - loss: 1.9988 - val_accuracy: 0.5187 - val_loss: 1.2978\n",
      "Epoch 2/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5244 - loss: 1.2796 - val_accuracy: 0.5882 - val_loss: 1.1272\n",
      "Epoch 3/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5935 - loss: 1.0781 - val_accuracy: 0.5989 - val_loss: 1.0187\n",
      "Epoch 4/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6354 - loss: 0.9878 - val_accuracy: 0.6043 - val_loss: 0.9891\n",
      "Epoch 5/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6503 - loss: 0.9058 - val_accuracy: 0.6203 - val_loss: 0.9548\n",
      "Epoch 6/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6919 - loss: 0.8334 - val_accuracy: 0.6364 - val_loss: 0.8573\n",
      "Epoch 7/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6895 - loss: 0.7672 - val_accuracy: 0.6578 - val_loss: 0.8313\n",
      "Epoch 8/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7021 - loss: 0.6844 - val_accuracy: 0.6524 - val_loss: 0.8358\n",
      "Epoch 9/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7269 - loss: 0.6230 - val_accuracy: 0.6898 - val_loss: 0.8294\n",
      "Epoch 10/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7576 - loss: 0.5744 - val_accuracy: 0.6845 - val_loss: 0.8320\n",
      "Epoch 11/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7322 - loss: 0.5800 - val_accuracy: 0.6684 - val_loss: 0.8225\n",
      "Epoch 12/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7506 - loss: 0.5585 - val_accuracy: 0.6578 - val_loss: 0.8162\n",
      "Epoch 13/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7729 - loss: 0.5536 - val_accuracy: 0.6364 - val_loss: 0.9892\n",
      "Epoch 14/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7543 - loss: 0.5613 - val_accuracy: 0.6631 - val_loss: 0.8844\n",
      "Epoch 15/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7701 - loss: 0.5171 - val_accuracy: 0.6738 - val_loss: 0.8891\n",
      "Epoch 16/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7509 - loss: 0.5787 - val_accuracy: 0.6684 - val_loss: 0.8400\n",
      "Epoch 17/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7933 - loss: 0.4969 - val_accuracy: 0.6684 - val_loss: 0.8386\n",
      "Epoch 18/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8221 - loss: 0.4340 - val_accuracy: 0.6738 - val_loss: 0.8720\n",
      "Epoch 19/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8212 - loss: 0.4291 - val_accuracy: 0.6578 - val_loss: 0.9321\n",
      "Epoch 20/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8277 - loss: 0.4177 - val_accuracy: 0.6791 - val_loss: 0.8592\n",
      "Epoch 21/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8164 - loss: 0.4374 - val_accuracy: 0.6898 - val_loss: 0.9309\n",
      "Epoch 22/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8158 - loss: 0.3966 - val_accuracy: 0.6898 - val_loss: 0.9581\n",
      "Epoch 23/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8609 - loss: 0.3382 - val_accuracy: 0.6738 - val_loss: 0.9492\n",
      "Epoch 24/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8773 - loss: 0.3237 - val_accuracy: 0.6845 - val_loss: 0.9659\n",
      "Epoch 25/25\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8546 - loss: 0.3466 - val_accuracy: 0.7059 - val_loss: 0.9849\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6773 - loss: 1.0164 \n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "# Supondo que dados_pca e df['Type'] estejam definidos corretamente\n",
    "unique_types = df['Type'].unique()\n",
    "num_classes = len(unique_types)\n",
    "\n",
    "# Convertendo unique_types para strings\n",
    "target_names = [str(cls) for cls in unique_types]\n",
    "\n",
    "# Divisão de treino e teste com random_state para reprodutibilidade\n",
    "X_train, X_test, y_train, y_test = train_test_split(dados_pca, df['Type'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Expansão das dimensões de X_train e X_test (somente uma vez)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Verificar as dimensões de X_train e X_test\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "\n",
    "# Função para criar o modelo CNN\n",
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(64, 3, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')  # num_classes deve ser o número de classes únicas\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',  # Use sparse_categorical_crossentropy se y_train não estiver one-hot\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Validação cruzada\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_scores = []\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train[train_index], X_train[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    model = create_cnn_model()\n",
    "    history = model.fit(X_fold_train, y_fold_train, epochs=25, batch_size=32, verbose=0, validation_data=(X_fold_val, y_fold_val))\n",
    "\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    cross_val_scores.append(val_accuracy)\n",
    "\n",
    "cross_val_mean = np.mean(cross_val_scores)\n",
    "cross_val_std = np.std(cross_val_scores)\n",
    "\n",
    "# Treinamento final da CNN\n",
    "model = create_cnn_model()\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Avaliação da CNN no conjunto de teste\n",
    "cnn_loss, cnn_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Exibindo resultados da CNN\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "val_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "# Predições da CNN no conjunto de teste\n",
    "cnn_predictions = model.predict(X_test)\n",
    "cnn_predictions_classes = np.argmax(cnn_predictions, axis=1)\n",
    "\n",
    "# Classification report da CNN\n",
    "cnn_report = classification_report(y_test, cnn_predictions_classes, target_names=target_names, zero_division=1, digits=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN - Acurácia (Treinamento): 0.8368983864784241\n",
      "CNN - Acurácia (Teste): 0.7058823704719543\n",
      "CNN - Validação Cruzada - Média: 0.7393378019332886 - Desvio Padrão: 0.018406364498221115\n",
      "Matriz de Confusão:\n",
      " [[79 15  1  2  0]\n",
      " [11 24  1  2  0]\n",
      " [ 2  1 23  1  2]\n",
      " [ 0  3  1  9  2]\n",
      " [ 4  0  0  2  2]]\n",
      "Classification Report CNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78        97\n",
      "           1       0.58      0.29      0.39        38\n",
      "           2       0.90      0.93      0.92        29\n",
      "           3       0.64      0.47      0.54        15\n",
      "           4       1.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.71       187\n",
      "   macro avg       0.76      0.52      0.52       187\n",
      "weighted avg       0.71      0.71      0.67       187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibindo resultados da CNN\n",
    "print(f'CNN - Acurácia (Treinamento): {train_accuracy}')\n",
    "print(f'CNN - Acurácia (Teste): {cnn_accuracy}')\n",
    "print(f'CNN - Validação Cruzada - Média: {cross_val_mean} - Desvio Padrão: {cross_val_std}')\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_test, cnn_predictions_test))\n",
    "\n",
    "# Classification report da CNN\n",
    "print(f'Classification Report CNN:\\n{cnn_report}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
